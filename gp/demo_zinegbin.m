%DEMO_ZINEGBIN  Demonstration of zero-inflated Negative-binomial model
%                 using Gaussian process prior
%
%  Description
%    The data used in the demonstration program is the same used by
%    Radford M. Neal in his three-way classification example in
%    Software for Flexible Bayesian Modeling
%    (http://www.cs.toronto.edu/~radford/fbm.software.html) The
%    data consists of 1000 4-D vectors which are classified into
%    three classes. The data is generated by drawing the components
%    of vector, x1, x2, x3 and x4, uniformly form (0,1). The class
%    of each vector is selected according to the first two
%    components of the vector, x_1 and x_2. After this a Gaussian
%    noise with standard deviation of 0.1 has been added to every
%    component of the vector. Because there are two irrelevant
%    components in the input vector a prior with ARD should be of
%    help.
%
%    The data is divided into two parts, trainig set of 400 units
%    and test set of 600 units.
%
%    The latent values for N training points and C classes are
%    f=(f1_1,f2_1,...,fN_1,f1_2,f2_2,...,fN_2,...,f1_C,f2_C,...,fN_C)^T,
%    and are given a zero mean Gaussian process prior
%      
%      f ~ N(0, K),
%
%    where K is a block diagonal covariance matrix with blocks
%    K_1,...,K_C whose elements are given by K_ij = k(x_i, x_j |
%    th). The function k(x_i, x_j | th) is covariance function and
%    th its parameters.
%
%    In this demo we approximate the posterior distribution with
%    Laplace approximation.
%

% Copyright (c) 2010 Jaakko Riihimï¿½ki, Jarno Vanhatalo, Aki Vehtari

% This software is distributed under the GNU General Public 
% License (version 2 or later); please refer to the file 
% License.txt, included with the software, for details.

% load the data
S = which('demo_spatial2');
data = load(strrep(S,'demo_spatial2.m','demos/spatial2.txt'));

x = data(:,1:2);
ye = data(:,3);
y = data(:,4);

ind=x(:,1)>40 & x(:,2)<40;
x=x(ind,:);
y=y(ind,:);
ye=ye(ind,:);

x0=x;
x=bsxfun(@rdivide,bsxfun(@minus,x0,mean(x0)),std(x0));

% Create the covariance functions
pl = prior_t('s2',10);
pm = prior_t('s2',10); 
% pm = prior_sqrtunif();
%gpcf1 = gpcf_sexp('lengthScale', [1 0.5], 'magnSigma2', 1.1, 'lengthScale_prior', pl, 'magnSigma2_prior', pm);
%gpcf2 = gpcf_sexp('lengthScale', 0.6, 'magnSigma2', 1.2, 'lengthScale_prior', pl, 'magnSigma2_prior', pm);
gpcf1 = gpcf_neuralnetwork('weightSigma2', [1 1], 'biasSigma2', 1, 'weightSigma2_prior', pl, 'biasSigma2_prior', pm);
gpcf2 = gpcf_neuralnetwork('weightSigma2', [1.2 2.1], 'biasSigma2', 0.8, 'weightSigma2_prior', pl, 'biasSigma2_prior', pm);
gpcf3 = gpcf_neuralnetwork('weightSigma2', [0.9 0.7], 'biasSigma2', 1.2, 'weightSigma2_prior', pl, 'biasSigma2_prior', pm);
%gpcf3 = gpcf_sexp('lengthScale', 1.15, 'magnSigma2', 1.05, 'lengthScale_prior', pl, 'magnSigma2_prior', pm);
%gpcf4 = gpcf_sexp('lengthScale', 1.25, 'magnSigma2', 1.2, 'lengthScale_prior', pl, 'magnSigma2_prior', pm);

% Create the likelihood structure
lik = lik_zinegbin();

% NOTE! if Multible covariance functions per latent is used, define
% gp.comp_cf as follows:
% gp.comp_cf = {[1 2] [3 4] [5 6]};
gp = gp_set('lik', lik, 'cf', {gpcf1 gpcf2 gpcf3}, 'jitterSigma2', 1e-6);
gp.comp_cf = {[1 2] [1 3]};

% Set the approximate inference method to Laplace
gp = gp_set(gp, 'latent_method', 'Laplace');

% Set the options for the scaled conjugate optimization
opt=optimset('TolFun',1e-4,'TolX',1e-4,'Display','iter','MaxIter',100,'Derivativecheck','on');
% Optimize with the scaled conjugate gradient method
gp=gp_optim(gp,x,y,'z',ye,'opt',opt);

% make the prediction for test points
[Eft, Varft, lpyt] = gp_pred(gp, x, y, x(1:10,:), 'z', ye, 'zt', ye(1:10,:), 'yt', y(1:10,:));

%w0=gp_pak(gp);
%gpla_nd_e(w0,gp,x,y,'z',ye)
%gpla_nd_g(w0,gp,x,y,'z',ye)
%gradcheck(randn(size(w0)),@gpla_nd_e,@gpla_nd_g,gp,x,y,'z',ye);
%break
%gradcheck(randn(size(w0)),@gpla_ml_e,@gpla_ml_g,gp,x,y,'z',ye);
%gradcheck(w0,@gpla_ml_e,@gpla_ml_g,gp,x,y,'z',ye);
